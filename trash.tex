
\section{An amortized scheme} \label{NEW.amortized-scheme}
In the analysis we found out that in our model the first $\Theta(\log n)$ bits are likely to be an almost unique identifier among $n$ elements. We will use $k$ to denote this number of significant bits.
We are going to use van Emde Boas tree (LVEBT) to represent the $k$ starting bits, which will give us $\OO(\log\log n)$ expected amortized time for any operation, but we still need to solve two problems.
One of them is the fact that $k$ needs to change during the life of the structure according to $n$ to satisfy Lemma~\ref{NEW.smooth-bucket-lemma}. The other problem is that we want the structure to work efficiently without knowing the parameters $\alpha$ and $\delta$.

\def\Half{\frac 1 2}

To remove dependency on the distribution's parameters, we simply choose a~growing function instead of the factor $\frac \alpha \delta$.
The time complexity of van Emde Boas trees is so low that we can afford to use a $\Theta(\log n)$ function, giving us $k \in \Theta(\log^2 n)$ significant bits.
More precisely, we will maintain $k \in \intervalOO{\Half \log^2 n}{2 \log^2 n}$.
When the structure is small ($\Half \log n < \frac \alpha \delta$), we might not fit into Lemma~\ref{NEW.smooth-bucket-lemma}, but that is only for inputs of constant size ($n < 2^{2 \alpha / \delta}$). It follows that the asymptotic complexities aren't affected and the expected size of any nonempty bucket is still $\OO(1)$.

Note that this kind of averaging for the size of buckets is different from the one generated by our dictionaries. In the bucketing, similarly to all methods in Chapter~\ref{IS.chapter}, there exist bad sets of input that always fall into the same bucket and thus they always cause the structure to perform slowly.
On the other hand, in our dictionaries we avoided such dependency on the input by utilising universal hashing in the implementation. In this way the dictionary operations were made to work in $\Theta(1)$ time on average for \emph{any set} of input values, only considering the average over the (pseudo)random bits.

To change $k$ we rebuild the structure periodically. We will always choose $k$ as a power of two and whenever $\log^2 n$ reaches a different power of two, we rebuild the whole structure with the new $k$. That will exactly ensure that we keep $k$ in the desired interval.

The rebuild clearly happens when the value of $\log^2 n$ changes from $\log^2 n_0$ to $2 \log^2 n_0$ or $\frac 1 2 \log^2 n_0$, that is when $n$ changes from $n_0$ to $n_0^{\sqrt 2}$ or $n_0^{1 / \sqrt 2}$.
It follows that during a rebuild, the number of modifying operations since the last rebuild is $\Omega(n)$, where $n$ is the current number of contained values.

We suppose that the rebuilding time is at most $\OO(n)$-times larger than the time needed for modifying operations, because otherwise we could simulate the rebuild by $\OO(n)$ \aMethod{FindMin, Delete} and \aMethod{Insert} operations.
It follows that the rebuilding time can be amortized into additional time that is asymptotically at most equal to the modification time.
As a consequence we can charge the rebuilding time to preceding modifying operations in a way that the expected amortized asymptotic times of modification operations aren't affected by the spread-out price for periodical rebuilds.
