
\section{Introduction \label{sec:defs}}

The predecessor problem involves maintaining a set of linearly
ordered keys and performing queries on that set. The basic variant allows insertions, deletions, and --- more complex to perform and extensively studied --- \emph{predecessor} query: find the largest contained key that is less than a given value. 

This paper investigates the predecessor problem on inputs described by  a well studied class of ``smooth'' probability distributions. 
In this section we formalize models of the input and computation employed
and introduce the class of smooth distributions. In~Section~\ref{sec:smooth-bucket} we
prove the key property of smooth distributions that seems to be
implicitly used in all related work (in weaker forms) as the most
important step to achieve the stated performance. In~Section~\ref{sec:main-results}
we show how to utilize this property directly with well-known distribution-independent
structures for the predecessor problem to get better performance in
a simpler way, essentially by converting to the case of polynomial-sized
universe.

We assume the usual word-RAM~\cite{HagerupT98}
as our computational model, and additionally we restrict keys to word-sized
integers. That can be shown not to be a~significant limitation, as
many other key types can be converted to the integer case cheaply,
for example standard floating-point \cite[sec. 2.1.3]{Goldberg91}
and string keys \cite{AndersT01}.

We study the behavior of the predecessor problem on specific input
key distributions, which requires us to specify how we model the input:
\begin{defn}[input model]
Insertions take keys distributed according to a~density~$\mu$ that
does not change during the whole life of the structure. Deletions
remove uniformly from the set of \emph{contained} elements. All the
operations are arbitrarily intermixed, and the keys are chosen independently
of each other.
\end{defn}
This model is convenient because it preserves distribution of the
stored set~â€“ at any point of execution the set appears to be a sample
taken from $\mu$. That is usually essential for simplifying complexity
analyses in related structures. Various random deletion models were
thoroughly studied by Knuth~\cite{Knuth77}.

Most of related papers work with key distributions that can be described
as \emph{$\left(s^{\alpha},s^{1-\delta}\right)$-smooth} for some
constants $\alpha,\delta>0$. Typically it is assumed that the particular
density of the distribution is not known, but $\alpha$ and $\delta$
are fixed parameters.
\begin{rem*}
If the distribution \emph{was} known and its inverse cumulative distribution
function $F^{-1}$ was cheaply computable (or approximable), we could
convert the problem to the uniform case (or another bounded density).
We could simply store keys transformed by $F^{-1}$, which would preserve
the order. With any bounded input distribution, all operations can
be easily handled in constant time by simply splitting the the key
domain into $\Theta(n)$ intervals and mapping each to an element
of an array.\footnote{We use $n$ to denote the current size of the stored set, following
the custom in data structures.} This solution for bounded distributions was pointed out already by
Andersson and Mattsson \cite[sec. 5.2]{AndersM93}.
\end{rem*}
The concept of a smooth probability density was introduced by Mehlhorn and 
\input{def-smooth.tex}
\begin{rem*}
This definition, as written, makes only sense for \emph{continuous} probability
distributions, yet any realistic models of computation can
\emph{not} assume handling arbitrary non-discrete values in constant
space and time per operation. Therefore we assume implicit rounding
when inserting a key, thus converting to the nearest value representable
in a single machine word. It seems reasonable that the related research
assumed something similar without stating it explicitly.
\end{rem*}

\section{Static analysis of bucketing}
 \label{sec:smooth-bucket}
In this section we show how smoothness implies that an arbitrary sufficiently
short interval is expected to get only a~constant number of input keys.
\begin{lem*}
\label{lem:smooth-bucket}
Given $\alpha,\delta>0$ and a positive
integer $n$, let us independently draw $n$ keys from a $\left(s^{\alpha},s^{1-\delta}\right)$-smooth
distribution, and split the whole domain into at least $n^{\alpha/\delta}$
equally long intervals. Then the expected number of keys in an~interval
is $\OO(1)$.
\end{lem*}
\input{smooth-lemma.tex}

\section{Combining with known results and comparing to related work \label{sec:main-results}}

We propose to split the bit representation of every key into two parts,
exactly as one step of of decomposition from van Emde Boas trees,
except that we split asymmetrically. In both parts we propose to use
standard structures that do not utilize distribution properties of
the input. The speed on the more significant bits will be due to the
keys being short, and for the less significant bits the number of
keys will be small in expected case due to smoothness.

\subsection{Reviewing standard vEBT decomposition}

Standard van Emde Boas trees solve the predecessor problem with all
operations needing $\OO(\log l)$ time when working with keys of $l$
bits. That assumes $l$ is not asymptotically larger than the word
length of the used word-RAM.

The well-known decomposition can be viewed as performing binary search
for the longest prefix of bit-representation that is shared by the
searched key and some of the stored keys. The structure stores a mapping
from the more significant halves to corresponding subsets of the less
significant halves of the stored keys. These subsets are stored recursively
in the same way, and also the set of occurring more significant halves
is stored recursively. The operations on vEBT are carefully designed
to perform at most one nontrivial recursive call on each recursion
level, and all the rest are constant-time operations.

To keep the space linear\footnote{In word-RAM data structures, linear space means using $\OO(n)$ words
of memory.}, some modifications to the original structure are required. The mappings
can be represented by hash tables instead of simple arrays, and indirection
can be used: the whole set is split into $\Theta(\log l)$-sized consecutive
clusters in a fashion similar to B-trees, and the actual vEBT construction
is only applied to the whole clusters joined together by a doubly
linked list. These changes make the complexity of insertions and deletions
expected and amortized where the expectation is only over random bits.

A possible way of doing these modifications is described in more details
e.g.~in~\cite[chapter 4]{Cunat10}, including the algorithms for
operations which can also be found in textbooks \cite[chapter 20]{intro2alg}.

\subsection{Modifying the vEBT decomposition}

To cover the cases most studied in previous work, we focus on using
linear space and work with $\left(s^{\alpha},s^{1-\delta}\right)$-smooth
distributions. To use the~\Cref{lem:smooth-bucket}, we need to split away at least
$(\alpha/\delta)\log n$ bits.\footnote{In case there are not that many bits in the word, we can ``split
all of them'', meaning we basically use vEB trees directly while
fitting into the stated complexity bounds.} If the values of parameters are unknown, we can split away more bits
to be asympotically certain, e.g. $\log^{2}n$.

These bounds on the number of bits change during the life of the structure,
but we can easily ensure that we cut at least that many by performing
a full rebuild after every $n_{0}/4$ modifying operations where $n_{0}$
denotes the size of the stored set during the last rebuild. It would
also be possible to deamortize that process by standard technique
of global rebuilding \cite{global-rebuilding}. Also note that $\left(\alpha/\delta\right)\log n>\log^{2}n$
only for $n<2^{\alpha/\delta}$ which is an unknown constant; thus
the performance will be constant in that base case.

We can use standard hashed vEBT on the more significant bits to get
$\OO\left(\log\left(\log^{2}n\right)\right)=\OO\left(\log\log n\right)$
expected amortized time per operation. Each substructure for the less
significant bits is expected to store $\OO(1)$ keys. The resulting
performance is essentially the same as that of~Kaporis et~al.~\cite{KMSTTZ06},
except that unlike all previous work we achieve linear space without
requiring $\alpha\le1$, and our approach is conceptually much simpler.
\begin{thm*}%[main result]
There is a structure for solving the predecessor problem in $\OO\left(\log\log n\right)$
expected time per operation and linear space on any $\left(s^{\alpha},s^{1-\delta}\right)$-smooth
distribution for arbitrary $\alpha,\delta>0$. The distribution and
parameters do not need to be known.
\end{thm*}

\subsection{Concluding remarks}

If we wanted to cut down time on unfavorable input, we could use the
same approach as~Kaporis et~al. There are standard structures for
the predecessor problem working in time $\OO(\!\sqrt{\log n/\log\log n})$
per operation on arbitrary input (considering the less precise bounds),
so these can be used for the less significant bits. Note that when
handling the more significant bits, the expected amortized time $\OO\left(\log\log n\right)$
remains independent of the input, as it only comes from randomization
during hashing.

We feel that it is good to decouple most of the analysis from complex
conditions on the input, as especially in practice we can rarely assume
that all operations on the structure are independent and identically
distributed. For the approach of our paper to work, it is enough (informally)
that the information is only carried by a sufficiently short bit prefix
of the keys.

\section*{Acknowledgements}

Most of the results presented above %in this paper 
were included in the author's master thesis~\cite{Cunat10}. The author wishes to dedicate this paper to the memory of %Many thanks belong to 
his supervisor V\'aclav Koubek who passed away before this work was completed.
%for helpful discussions during our research into this problem.



